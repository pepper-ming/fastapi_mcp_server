# FastAPI MCP Server 第二週任務細分計畫

## 總體目標

基於第一週已完成的基礎架構，進行技術架構升級與核心功能擴展。重點修正第一週實作與企畫書的技術差異，並開始實作時間序列分析、進階統計功能和資料庫整合。

## 前置檢討：第一週實作與企畫書差異修正

### 重要技術債務
1. **MCP 整合方式不一致**
   - 企畫書：使用官方 `fastapi-mcp==0.3.0`
   - 實際可用：`tadata-org/fastapi_mcp`
   - 當前實作：自訂 MCP 整合（已可運作）

2. **解決策略**
   - 保持當前可運作的實作為基礎
   - 逐步遷移至標準 fastapi-mcp 套件
   - 確保向下相容性

## 任務細分（第 8-14 天）

### 第 8 天：MCP 整合標準化與性能優化

#### 任務 8.1：評估並遷移至標準 fastapi-mcp 套件
**預計時間：2.5 小時**

```bash
# 安裝標準 fastapi-mcp 套件
poetry add fastapi-mcp

# 備份當前實作
cp app/main.py app/main_backup.py
```

```python
# app/main_v2.py - 新版 MCP 整合
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP

from app.core.settings import get_settings
from app.api.statistics import router as stats_router

settings = get_settings()

# 建立 FastAPI 應用程式
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="基於 Model Context Protocol 的統計分析與機器學習推論服務平台",
    debug=settings.debug,
)

# 添加 CORS 中介軟體
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=settings.cors_allow_credentials,
    allow_methods=settings.cors_allow_methods,
    allow_headers=settings.cors_allow_headers,
)

# 註冊路由
app.include_router(stats_router)

# 使用標準 fastapi-mcp 整合
mcp = FastApiMCP(
    app,
    name=settings.mcp_name,
    description=settings.mcp_description,
    # 只暴露統計相關端點
    include_tags=["統計分析"],
    describe_all_responses=True,
)

# 基礎路由
@app.get("/", tags=["基礎"])
async def root():
    """根端點 - 服務狀態"""
    return {
        "service": settings.app_name,
        "version": settings.app_version,
        "status": "running",
        "mcp_endpoint": "/mcp"
    }

@app.get("/health", tags=["基礎"])
async def health_check():
    """健康檢查端點"""
    return {
        "status": "healthy",
        "service": settings.app_name,
        "version": settings.app_version
    }

# 掛載 MCP 服務
mcp.mount()  # 使用正確的掛載方法

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.reload,
        log_level="debug" if settings.debug else "info",
    )
```

#### 任務 8.2：MCP 工具暴露策略優化
**預計時間：1 小時**

```python
# app/core/mcp_config.py
from typing import List, Optional
from pydantic import BaseModel

class MCPToolConfig(BaseModel):
    """MCP 工具暴露配置"""

    # 包含的操作 ID
    include_operations: Optional[List[str]] = [
        "calculate_descriptive_statistics",
        "perform_hypothesis_test",
        "get_supported_tests"
    ]

    # 包含的標籤
    include_tags: Optional[List[str]] = ["統計分析"]

    # 排除的標籤
    exclude_tags: Optional[List[str]] = ["內部", "測試"]

    # 是否包含完整回應架構描述
    describe_all_responses: bool = True

    # 是否包含完整 JSON 架構
    describe_full_response_schema: bool = False
```

#### 任務 8.3：MCP 性能監控與日誌
**預計時間：30 分鐘**

```python
# app/middleware/mcp_monitoring.py
import time
import logging
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

logger = logging.getLogger(__name__)

class MCPMonitoringMiddleware(BaseHTTPMiddleware):
    """MCP 請求監控中介軟體"""

    async def dispatch(self, request: Request, call_next):
        # 記錄 MCP 相關請求
        if request.url.path.startswith("/mcp"):
            start_time = time.time()

            logger.info(
                f"MCP request started: {request.method} {request.url.path}",
                extra={
                    "mcp_request": True,
                    "method": request.method,
                    "path": request.url.path,
                    "client_ip": request.client.host
                }
            )

            response: Response = await call_next(request)

            process_time = time.time() - start_time
            logger.info(
                f"MCP request completed: {response.status_code} in {process_time:.3f}s",
                extra={
                    "mcp_request": True,
                    "status_code": response.status_code,
                    "process_time": process_time
                }
            )

            return response

        return await call_next(request)
```

### 第 9 天：資料庫架構設計與實作

#### 任務 9.1：TimescaleDB 容器化設定
**預計時間：1.5 小時**

```yaml
# docker-compose.timescale.yml
version: '3.8'

services:
  timescaledb:
    image: timescale/timescaledb:2.14-pg16
    container_name: mcp-timescaledb
    restart: unless-stopped
    environment:
      - POSTGRES_DB=fastapi_mcp
      - POSTGRES_USER=mcp_user
      - POSTGRES_PASSWORD=mcp_password
    ports:
      - "5432:5432"
    volumes:
      - timescale_data:/var/lib/postgresql/data
      - ./scripts/init_timescale.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - mcp_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mcp_user -d fastapi_mcp"]
      interval: 30s
      timeout: 10s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: mcp-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - mcp_network
    command: redis-server --appendonly yes

volumes:
  timescale_data:
  redis_data:

networks:
  mcp_network:
    driver: bridge
```

```sql
-- scripts/init_timescale.sql
-- 初始化 TimescaleDB

-- 建立時間序列分析相關表
CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;

-- 統計分析結果表
CREATE TABLE analysis_results (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    analysis_type VARCHAR(50) NOT NULL,
    input_data JSONB NOT NULL,
    results JSONB NOT NULL,
    execution_time_ms INTEGER,
    user_id VARCHAR(100),
    session_id VARCHAR(100)
);

-- 建立時間序列超表
SELECT create_hypertable('analysis_results', 'timestamp');

-- 時間序列資料表
CREATE TABLE timeseries_data (
    timestamp TIMESTAMPTZ NOT NULL,
    series_id VARCHAR(100) NOT NULL,
    value DOUBLE PRECISION NOT NULL,
    metadata JSONB,
    PRIMARY KEY (timestamp, series_id)
);

SELECT create_hypertable('timeseries_data', 'timestamp');

-- 模型訓練記錄表
CREATE TABLE model_training_logs (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    training_data_hash VARCHAR(64),
    hyperparameters JSONB,
    metrics JSONB,
    model_path VARCHAR(200),
    status VARCHAR(20) DEFAULT 'training'
);

SELECT create_hypertable('model_training_logs', 'timestamp');

-- 建立索引
CREATE INDEX idx_analysis_type ON analysis_results (analysis_type, timestamp DESC);
CREATE INDEX idx_series_id ON timeseries_data (series_id, timestamp DESC);
CREATE INDEX idx_model_name ON model_training_logs (model_name, timestamp DESC);
```

#### 任務 9.2：資料庫抽象層設計
**預計時間：2 小時**

```python
# app/database/base.py
from typing import AsyncGenerator
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy import MetaData

from app.core.settings import get_settings

settings = get_settings()

# 非同步資料庫引擎
engine = create_async_engine(
    settings.database_url,
    echo=settings.database_echo,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20,
)

# 非同步 Session 工廠
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
)

# 基礎模型類別
class Base(DeclarativeBase):
    metadata = MetaData()

# 依賴注入用的資料庫 Session
async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()
```

```python
# app/models/database.py
import uuid
from datetime import datetime
from typing import Optional, Dict, Any
from sqlalchemy import Column, Integer, String, DateTime, Text, JSON, Float
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.sql import func

from app.database.base import Base

class AnalysisResult(Base):
    """統計分析結果模型"""
    __tablename__ = "analysis_results"

    id = Column(Integer, primary_key=True, index=True)
    timestamp = Column(DateTime(timezone=True), server_default=func.now(), index=True)
    analysis_type = Column(String(50), nullable=False, index=True)
    input_data = Column(JSONB, nullable=False)
    results = Column(JSONB, nullable=False)
    execution_time_ms = Column(Integer)
    user_id = Column(String(100), index=True)
    session_id = Column(String(100), index=True)

class TimeSeriesData(Base):
    """時間序列資料模型"""
    __tablename__ = "timeseries_data"

    timestamp = Column(DateTime(timezone=True), primary_key=True)
    series_id = Column(String(100), primary_key=True, index=True)
    value = Column(Float, nullable=False)
    metadata = Column(JSONB)

class ModelTrainingLog(Base):
    """模型訓練日誌模型"""
    __tablename__ = "model_training_logs"

    id = Column(Integer, primary_key=True, index=True)
    timestamp = Column(DateTime(timezone=True), server_default=func.now(), index=True)
    model_name = Column(String(100), nullable=False, index=True)
    model_version = Column(String(50))
    training_data_hash = Column(String(64))
    hyperparameters = Column(JSONB)
    metrics = Column(JSONB)
    model_path = Column(String(200))
    status = Column(String(20), default='training')
```

#### 任務 9.3：資料庫遷移系統設定
**預計時間：30 分鐘**

```bash
# 安裝 Alembic
poetry add alembic

# 初始化 Alembic
poetry run alembic init alembic
```

```python
# alembic/env.py 配置
import asyncio
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from sqlalchemy.ext.asyncio import AsyncEngine
from alembic import context

from app.database.base import Base
from app.core.settings import get_settings

config = context.config
settings = get_settings()

# 設定資料庫 URL
config.set_main_option("sqlalchemy.url", settings.database_url)

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection):
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        context.run_migrations()

async def run_migrations_online() -> None:
    connectable = AsyncEngine(
        engine_from_config(
            config.get_section(config.config_ini_section),
            prefix="sqlalchemy.",
            poolclass=pool.NullPool,
        )
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

if context.is_offline_mode():
    run_migrations_offline()
else:
    asyncio.run(run_migrations_online())
```

### 第 10 天：進階統計分析功能

#### 任務 10.1：相關性分析與迴歸模組
**預計時間：2.5 小時**

```python
# app/models/advanced_statistics.py
from typing import List, Optional, Union, Dict, Any
from pydantic import BaseModel, Field
from enum import Enum

class CorrelationType(str, Enum):
    """相關性分析類型"""
    PEARSON = "pearson"
    SPEARMAN = "spearman"
    KENDALL = "kendall"

class RegressionType(str, Enum):
    """迴歸分析類型"""
    LINEAR = "linear"
    POLYNOMIAL = "polynomial"
    LOGISTIC = "logistic"
    RIDGE = "ridge"
    LASSO = "lasso"

class CorrelationRequest(BaseModel):
    """相關性分析請求"""
    x_data: List[Union[int, float]] = Field(..., description="X 變數資料", min_items=3)
    y_data: List[Union[int, float]] = Field(..., description="Y 變數資料", min_items=3)
    correlation_type: CorrelationType = Field(default=CorrelationType.PEARSON, description="相關性分析類型")
    alpha: float = Field(default=0.05, ge=0.001, le=0.1, description="顯著水準")

class CorrelationResult(BaseModel):
    """相關性分析結果"""
    correlation_coefficient: float = Field(description="相關係數")
    p_value: float = Field(description="p 值")
    confidence_interval: Dict[str, float] = Field(description="信賴區間")
    interpretation: str = Field(description="結果詮釋")
    strength: str = Field(description="相關強度")

class RegressionRequest(BaseModel):
    """迴歸分析請求"""
    x_data: List[Union[int, float]] = Field(..., description="自變數資料", min_items=3)
    y_data: List[Union[int, float]] = Field(..., description="依變數資料", min_items=3)
    regression_type: RegressionType = Field(default=RegressionType.LINEAR, description="迴歸類型")
    polynomial_degree: Optional[int] = Field(default=2, ge=2, le=5, description="多項式次數")
    alpha: float = Field(default=0.05, description="正則化參數 (Ridge/Lasso)")

class RegressionResult(BaseModel):
    """迴歸分析結果"""
    coefficients: List[float] = Field(description="迴歸係數")
    intercept: float = Field(description="截距")
    r_squared: float = Field(description="決定係數")
    adjusted_r_squared: float = Field(description="調整後決定係數")
    f_statistic: float = Field(description="F 統計量")
    f_p_value: float = Field(description="F 檢定 p 值")
    residual_analysis: Dict[str, Any] = Field(description="殘差分析")
    prediction_equation: str = Field(description="預測方程式")
```

```python
# app/services/advanced_statistics.py
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
from typing import Tuple, Dict, Any

from app.models.advanced_statistics import (
    CorrelationRequest, CorrelationResult,
    RegressionRequest, RegressionResult,
    CorrelationType, RegressionType
)

class AdvancedStatisticsService:
    """進階統計分析服務"""

    @staticmethod
    def calculate_correlation(request: CorrelationRequest) -> CorrelationResult:
        """計算相關性分析"""

        x = np.array(request.x_data)
        y = np.array(request.y_data)

        if len(x) != len(y):
            raise ValueError("X 和 Y 資料長度必須相同")

        # 計算相關係數
        if request.correlation_type == CorrelationType.PEARSON:
            corr_coef, p_value = stats.pearsonr(x, y)
            method_name = "Pearson"
        elif request.correlation_type == CorrelationType.SPEARMAN:
            corr_coef, p_value = stats.spearmanr(x, y)
            method_name = "Spearman"
        elif request.correlation_type == CorrelationType.KENDALL:
            corr_coef, p_value = stats.kendalltau(x, y)
            method_name = "Kendall"
        else:
            raise ValueError(f"不支援的相關性分析類型: {request.correlation_type}")

        # 計算信賴區間 (僅適用於 Pearson)
        if request.correlation_type == CorrelationType.PEARSON:
            n = len(x)
            z_score = stats.norm.ppf(1 - request.alpha / 2)
            se = 1 / np.sqrt(n - 3)
            z_r = 0.5 * np.log((1 + corr_coef) / (1 - corr_coef))
            ci_lower = np.tanh(z_r - z_score * se)
            ci_upper = np.tanh(z_r + z_score * se)
        else:
            ci_lower, ci_upper = None, None

        # 判定相關強度
        abs_corr = abs(corr_coef)
        if abs_corr < 0.3:
            strength = "弱相關"
        elif abs_corr < 0.7:
            strength = "中等相關"
        else:
            strength = "強相關"

        # 結果詮釋
        direction = "正" if corr_coef > 0 else "負"
        significance = "顯著" if p_value < request.alpha else "不顯著"
        interpretation = f"{method_name} 相關性分析顯示 {direction}相關 (r = {corr_coef:.4f})，在 α = {request.alpha} 水準下{significance}"

        return CorrelationResult(
            correlation_coefficient=float(corr_coef),
            p_value=float(p_value),
            confidence_interval={
                "lower": float(ci_lower) if ci_lower is not None else None,
                "upper": float(ci_upper) if ci_upper is not None else None,
                "level": 1 - request.alpha
            },
            interpretation=interpretation,
            strength=strength
        )

    @staticmethod
    def perform_regression(request: RegressionRequest) -> RegressionResult:
        """執行迴歸分析"""

        x = np.array(request.x_data).reshape(-1, 1)
        y = np.array(request.y_data)

        if len(x) != len(y):
            raise ValueError("X 和 Y 資料長度必須相同")

        # 選擇迴歸模型
        if request.regression_type == RegressionType.LINEAR:
            model = LinearRegression()
            x_processed = x

        elif request.regression_type == RegressionType.POLYNOMIAL:
            poly_features = PolynomialFeatures(degree=request.polynomial_degree)
            x_processed = poly_features.fit_transform(x)
            model = LinearRegression()

        elif request.regression_type == RegressionType.RIDGE:
            model = Ridge(alpha=request.alpha)
            x_processed = x

        elif request.regression_type == RegressionType.LASSO:
            model = Lasso(alpha=request.alpha)
            x_processed = x

        elif request.regression_type == RegressionType.LOGISTIC:
            model = LogisticRegression()
            x_processed = x
        else:
            raise ValueError(f"不支援的迴歸類型: {request.regression_type}")

        # 訓練模型
        model.fit(x_processed, y)

        # 預測
        y_pred = model.predict(x_processed)

        # 計算統計量
        if request.regression_type != RegressionType.LOGISTIC:
            r2 = r2_score(y, y_pred)
            n = len(y)
            p = x_processed.shape[1]  # 參數個數
            adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)

            # F 統計量
            mse_model = mean_squared_error(y, y_pred)
            mse_total = np.var(y)
            f_stat = ((mse_total - mse_model) / p) / (mse_model / (n - p - 1))
            f_p_value = 1 - stats.f.cdf(f_stat, p, n - p - 1)
        else:
            # 邏輯迴歸使用不同的評估指標
            r2 = model.score(x_processed, y)
            adjusted_r2 = r2  # 簡化處理
            f_stat = 0
            f_p_value = 1

        # 殘差分析
        if request.regression_type != RegressionType.LOGISTIC:
            residuals = y - y_pred
            residual_analysis = {
                "mean_residual": float(np.mean(residuals)),
                "std_residual": float(np.std(residuals)),
                "shapiro_wilk_p": float(stats.shapiro(residuals)[1]),
                "durbin_watson": float(AdvancedStatisticsService._durbin_watson(residuals))
            }
        else:
            residual_analysis = {"note": "邏輯迴歸不適用殘差分析"}

        # 預測方程式
        if hasattr(model, 'coef_'):
            coef = model.coef_
            intercept = model.intercept_

            if request.regression_type == RegressionType.LINEAR:
                equation = f"Y = {intercept:.4f} + {coef[0]:.4f} * X"
            elif request.regression_type == RegressionType.POLYNOMIAL:
                terms = [f"{intercept:.4f}"]
                for i, c in enumerate(coef):
                    if i == 0:
                        terms.append(f"{c:.4f} * X")
                    else:
                        terms.append(f"{c:.4f} * X^{i+1}")
                equation = "Y = " + " + ".join(terms)
            else:
                equation = f"Y = {intercept:.4f} + {coef[0]:.4f} * X"
        else:
            equation = "方程式不可用"

        return RegressionResult(
            coefficients=coef.tolist() if hasattr(model, 'coef_') else [],
            intercept=float(intercept) if hasattr(model, 'intercept_') else 0.0,
            r_squared=float(r2),
            adjusted_r_squared=float(adjusted_r2),
            f_statistic=float(f_stat),
            f_p_value=float(f_p_value),
            residual_analysis=residual_analysis,
            prediction_equation=equation
        )

    @staticmethod
    def _durbin_watson(residuals: np.ndarray) -> float:
        """計算 Durbin-Watson 統計量"""
        diff = np.diff(residuals)
        return np.sum(diff**2) / np.sum(residuals**2)
```

#### 任務 10.2：進階統計 API 端點
**預計時間：1 小時**

```python
# app/api/advanced_statistics.py
from fastapi import APIRouter, HTTPException
from app.models.advanced_statistics import (
    CorrelationRequest, CorrelationResult,
    RegressionRequest, RegressionResult
)
from app.services.advanced_statistics import AdvancedStatisticsService

router = APIRouter(prefix="/statistics/advanced", tags=["進階統計分析"])

@router.post(
    "/correlation",
    response_model=CorrelationResult,
    operation_id="calculate_correlation",
    summary="相關性分析",
    description="計算兩變數間的相關性，支援 Pearson、Spearman、Kendall 相關係數"
)
async def calculate_correlation(request: CorrelationRequest) -> CorrelationResult:
    """計算相關性分析"""
    try:
        return AdvancedStatisticsService.calculate_correlation(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"相關性分析時發生錯誤: {str(e)}"
        )

@router.post(
    "/regression",
    response_model=RegressionResult,
    operation_id="perform_regression",
    summary="迴歸分析",
    description="執行各種迴歸分析，包含線性、多項式、Ridge、Lasso 迴歸"
)
async def perform_regression(request: RegressionRequest) -> RegressionResult:
    """執行迴歸分析"""
    try:
        return AdvancedStatisticsService.perform_regression(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"迴歸分析時發生錯誤: {str(e)}"
        )
```

### 第 11 天：時間序列分析基礎

#### 任務 11.1：時間序列資料模型設計
**預計時間：2 小時**

```python
# app/models/timeseries.py
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field
from enum import Enum

class TimeSeriesFrequency(str, Enum):
    """時間序列頻率"""
    DAILY = "D"
    WEEKLY = "W"
    MONTHLY = "M"
    QUARTERLY = "Q"
    YEARLY = "Y"
    HOURLY = "H"

class ForecastModel(str, Enum):
    """預測模型類型"""
    ARIMA = "arima"
    EXPONENTIAL_SMOOTHING = "exponential_smoothing"
    LINEAR_TREND = "linear_trend"
    MOVING_AVERAGE = "moving_average"

class TimeSeriesPoint(BaseModel):
    """時間序列資料點"""
    timestamp: datetime = Field(..., description="時間戳記")
    value: float = Field(..., description="數值")

class TimeSeriesData(BaseModel):
    """時間序列資料"""
    series_id: str = Field(..., description="序列識別碼")
    data: List[TimeSeriesPoint] = Field(..., description="時間序列資料點", min_items=10)
    frequency: Optional[TimeSeriesFrequency] = Field(default=None, description="資料頻率")
    metadata: Optional[Dict[str, Any]] = Field(default={}, description="額外資訊")

class ForecastRequest(BaseModel):
    """預測請求"""
    timeseries: TimeSeriesData = Field(..., description="時間序列資料")
    model_type: ForecastModel = Field(default=ForecastModel.LINEAR_TREND, description="預測模型")
    forecast_periods: int = Field(default=10, ge=1, le=100, description="預測期數")
    confidence_level: float = Field(default=0.95, ge=0.01, le=0.99, description="信賴水準")

    # ARIMA 參數
    arima_order: Optional[tuple] = Field(default=(1, 1, 1), description="ARIMA 階數 (p,d,q)")

    # 移動平均參數
    ma_window: Optional[int] = Field(default=5, ge=2, le=20, description="移動平均視窗")

class ForecastResult(BaseModel):
    """預測結果"""
    series_id: str = Field(description="序列識別碼")
    model_type: str = Field(description="使用的模型類型")
    forecast_values: List[float] = Field(description="預測值")
    forecast_dates: List[datetime] = Field(description="預測時間點")
    confidence_intervals: List[Dict[str, float]] = Field(description="信賴區間")
    model_metrics: Dict[str, float] = Field(description="模型評估指標")
    model_summary: str = Field(description="模型摘要")

class AnomalyDetectionRequest(BaseModel):
    """異常檢測請求"""
    timeseries: TimeSeriesData = Field(..., description="時間序列資料")
    detection_method: str = Field(default="statistical", description="檢測方法: statistical, iqr, zscore")
    sensitivity: float = Field(default=2.0, ge=1.0, le=5.0, description="敏感度")
    window_size: Optional[int] = Field(default=10, ge=5, le=50, description="滑動視窗大小")

class AnomalyPoint(BaseModel):
    """異常點"""
    timestamp: datetime = Field(description="異常時間點")
    value: float = Field(description="異常值")
    anomaly_score: float = Field(description="異常分數")
    expected_range: Dict[str, float] = Field(description="預期範圍")

class AnomalyDetectionResult(BaseModel):
    """異常檢測結果"""
    series_id: str = Field(description="序列識別碼")
    detection_method: str = Field(description="檢測方法")
    anomaly_points: List[AnomalyPoint] = Field(description="異常點列表")
    anomaly_rate: float = Field(description="異常率")
    summary: str = Field(description="檢測摘要")
```

#### 任務 11.2：時間序列分析服務實作
**預計時間：2.5 小時**

```python
# app/services/timeseries.py
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Any
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

from app.models.timeseries import (
    TimeSeriesData, ForecastRequest, ForecastResult,
    AnomalyDetectionRequest, AnomalyDetectionResult, AnomalyPoint,
    ForecastModel
)

class TimeSeriesService:
    """時間序列分析服務"""

    @staticmethod
    def forecast_timeseries(request: ForecastRequest) -> ForecastResult:
        """時間序列預測"""

        # 準備資料
        df = TimeSeriesService._prepare_dataframe(request.timeseries)

        if request.model_type == ForecastModel.LINEAR_TREND:
            return TimeSeriesService._linear_trend_forecast(request, df)
        elif request.model_type == ForecastModel.MOVING_AVERAGE:
            return TimeSeriesService._moving_average_forecast(request, df)
        elif request.model_type == ForecastModel.EXPONENTIAL_SMOOTHING:
            return TimeSeriesService._exponential_smoothing_forecast(request, df)
        elif request.model_type == ForecastModel.ARIMA:
            return TimeSeriesService._arima_forecast(request, df)
        else:
            raise ValueError(f"不支援的預測模型: {request.model_type}")

    @staticmethod
    def detect_anomalies(request: AnomalyDetectionRequest) -> AnomalyDetectionResult:
        """異常檢測"""

        df = TimeSeriesService._prepare_dataframe(request.timeseries)

        if request.detection_method == "statistical":
            anomalies = TimeSeriesService._statistical_anomaly_detection(df, request.sensitivity)
        elif request.detection_method == "iqr":
            anomalies = TimeSeriesService._iqr_anomaly_detection(df, request.sensitivity)
        elif request.detection_method == "zscore":
            anomalies = TimeSeriesService._zscore_anomaly_detection(df, request.sensitivity)
        else:
            raise ValueError(f"不支援的異常檢測方法: {request.detection_method}")

        anomaly_rate = len(anomalies) / len(df) if len(df) > 0 else 0

        return AnomalyDetectionResult(
            series_id=request.timeseries.series_id,
            detection_method=request.detection_method,
            anomaly_points=anomalies,
            anomaly_rate=anomaly_rate,
            summary=f"使用 {request.detection_method} 方法檢測到 {len(anomalies)} 個異常點，異常率為 {anomaly_rate:.2%}"
        )

    @staticmethod
    def _prepare_dataframe(timeseries: TimeSeriesData) -> pd.DataFrame:
        """準備 pandas DataFrame"""
        data = []
        for point in timeseries.data:
            data.append({
                'timestamp': point.timestamp,
                'value': point.value
            })

        df = pd.DataFrame(data)
        df = df.sort_values('timestamp')
        df = df.reset_index(drop=True)
        return df

    @staticmethod
    def _linear_trend_forecast(request: ForecastRequest, df: pd.DataFrame) -> ForecastResult:
        """線性趨勢預測"""

        # 準備特徵 (時間轉換為數值)
        df['time_numeric'] = pd.to_numeric(df['timestamp'])
        X = df['time_numeric'].values.reshape(-1, 1)
        y = df['value'].values

        # 訓練線性回歸模型
        model = LinearRegression()
        model.fit(X, y)

        # 生成預測時間點
        last_time = df['timestamp'].iloc[-1]
        time_diff = df['timestamp'].iloc[-1] - df['timestamp'].iloc[-2]
        forecast_dates = []
        for i in range(1, request.forecast_periods + 1):
            forecast_dates.append(last_time + i * time_diff)

        # 預測
        forecast_times_numeric = pd.to_numeric(pd.Series(forecast_dates)).values.reshape(-1, 1)
        forecast_values = model.predict(forecast_times_numeric)

        # 計算信賴區間
        y_pred = model.predict(X)
        mse = mean_squared_error(y, y_pred)
        std_error = np.sqrt(mse)

        z_score = stats.norm.ppf(1 - (1 - request.confidence_level) / 2)
        confidence_intervals = []
        for val in forecast_values:
            confidence_intervals.append({
                'lower': val - z_score * std_error,
                'upper': val + z_score * std_error
            })

        # 模型評估
        mae = mean_absolute_error(y, y_pred)
        r2 = model.score(X, y)

        return ForecastResult(
            series_id=request.timeseries.series_id,
            model_type="linear_trend",
            forecast_values=forecast_values.tolist(),
            forecast_dates=forecast_dates,
            confidence_intervals=confidence_intervals,
            model_metrics={
                'mae': mae,
                'mse': mse,
                'r2': r2,
                'rmse': np.sqrt(mse)
            },
            model_summary=f"線性趨勢模型 (R² = {r2:.4f}, RMSE = {np.sqrt(mse):.4f})"
        )

    @staticmethod
    def _moving_average_forecast(request: ForecastRequest, df: pd.DataFrame) -> ForecastResult:
        """移動平均預測"""

        values = df['value'].values
        window = request.ma_window

        # 計算移動平均
        if len(values) < window:
            raise ValueError(f"資料點數量 ({len(values)}) 少於移動平均視窗 ({window})")

        # 最近 window 個點的平均值作為預測值
        recent_values = values[-window:]
        forecast_value = np.mean(recent_values)

        # 生成預測時間點和值
        last_time = df['timestamp'].iloc[-1]
        time_diff = df['timestamp'].iloc[-1] - df['timestamp'].iloc[-2]
        forecast_dates = []
        forecast_values = []

        for i in range(1, request.forecast_periods + 1):
            forecast_dates.append(last_time + i * time_diff)
            forecast_values.append(forecast_value)  # 簡化處理：所有預測值相同

        # 計算標準誤差
        std_error = np.std(recent_values)
        z_score = stats.norm.ppf(1 - (1 - request.confidence_level) / 2)

        confidence_intervals = []
        for val in forecast_values:
            confidence_intervals.append({
                'lower': val - z_score * std_error,
                'upper': val + z_score * std_error
            })

        return ForecastResult(
            series_id=request.timeseries.series_id,
            model_type="moving_average",
            forecast_values=forecast_values,
            forecast_dates=forecast_dates,
            confidence_intervals=confidence_intervals,
            model_metrics={
                'window_size': window,
                'forecast_std': std_error
            },
            model_summary=f"{window} 期移動平均預測"
        )

    @staticmethod
    def _exponential_smoothing_forecast(request: ForecastRequest, df: pd.DataFrame) -> ForecastResult:
        """指數平滑預測"""

        values = df['value'].values
        alpha = 0.3  # 平滑參數

        # 簡單指數平滑
        smoothed = [values[0]]
        for i in range(1, len(values)):
            smoothed.append(alpha * values[i] + (1 - alpha) * smoothed[-1])

        # 預測值為最後一個平滑值
        forecast_value = smoothed[-1]

        # 生成預測時間點和值
        last_time = df['timestamp'].iloc[-1]
        time_diff = df['timestamp'].iloc[-1] - df['timestamp'].iloc[-2]
        forecast_dates = []
        forecast_values = []

        for i in range(1, request.forecast_periods + 1):
            forecast_dates.append(last_time + i * time_diff)
            forecast_values.append(forecast_value)

        # 計算殘差標準差
        residuals = values[1:] - smoothed[:-1]
        std_error = np.std(residuals)
        z_score = stats.norm.ppf(1 - (1 - request.confidence_level) / 2)

        confidence_intervals = []
        for val in forecast_values:
            confidence_intervals.append({
                'lower': val - z_score * std_error,
                'upper': val + z_score * std_error
            })

        return ForecastResult(
            series_id=request.timeseries.series_id,
            model_type="exponential_smoothing",
            forecast_values=forecast_values,
            forecast_dates=forecast_dates,
            confidence_intervals=confidence_intervals,
            model_metrics={
                'alpha': alpha,
                'residual_std': std_error
            },
            model_summary=f"指數平滑預測 (α = {alpha})"
        )

    @staticmethod
    def _arima_forecast(request: ForecastRequest, df: pd.DataFrame) -> ForecastResult:
        """ARIMA 預測 (簡化版本)"""
        # 注意：這是一個簡化的 ARIMA 實作
        # 在實際應用中建議使用 statsmodels 或 pmdarima

        values = df['value'].values

        # 簡化處理：使用一階差分和移動平均
        if len(values) < 10:
            raise ValueError("ARIMA 模型需要至少 10 個資料點")

        # 一階差分
        diff_values = np.diff(values)

        # 預測差分值 (使用最近 3 個差分值的平均)
        recent_diffs = diff_values[-3:]
        forecast_diff = np.mean(recent_diffs)

        # 生成預測
        last_value = values[-1]
        forecast_values = []
        current_value = last_value

        for _ in range(request.forecast_periods):
            current_value = current_value + forecast_diff
            forecast_values.append(current_value)

        # 生成預測時間點
        last_time = df['timestamp'].iloc[-1]
        time_diff = df['timestamp'].iloc[-1] - df['timestamp'].iloc[-2]
        forecast_dates = []

        for i in range(1, request.forecast_periods + 1):
            forecast_dates.append(last_time + i * time_diff)

        # 簡化的信賴區間
        std_error = np.std(diff_values)
        z_score = stats.norm.ppf(1 - (1 - request.confidence_level) / 2)

        confidence_intervals = []
        for i, val in enumerate(forecast_values):
            # 誤差隨預測期增加而增大
            adjusted_std = std_error * np.sqrt(i + 1)
            confidence_intervals.append({
                'lower': val - z_score * adjusted_std,
                'upper': val + z_score * adjusted_std
            })

        return ForecastResult(
            series_id=request.timeseries.series_id,
            model_type="arima",
            forecast_values=forecast_values,
            forecast_dates=forecast_dates,
            confidence_intervals=confidence_intervals,
            model_metrics={
                'order': request.arima_order,
                'diff_std': std_error
            },
            model_summary=f"ARIMA{request.arima_order} 預測 (簡化版本)"
        )

    @staticmethod
    def _statistical_anomaly_detection(df: pd.DataFrame, sensitivity: float) -> List[AnomalyPoint]:
        """統計方法異常檢測"""
        values = df['value'].values
        mean_val = np.mean(values)
        std_val = np.std(values)

        threshold = sensitivity * std_val
        anomalies = []

        for i, (_, row) in enumerate(df.iterrows()):
            deviation = abs(row['value'] - mean_val)
            if deviation > threshold:
                anomalies.append(AnomalyPoint(
                    timestamp=row['timestamp'],
                    value=row['value'],
                    anomaly_score=deviation / std_val,
                    expected_range={
                        'lower': mean_val - threshold,
                        'upper': mean_val + threshold
                    }
                ))

        return anomalies

    @staticmethod
    def _iqr_anomaly_detection(df: pd.DataFrame, sensitivity: float) -> List[AnomalyPoint]:
        """IQR 方法異常檢測"""
        values = df['value'].values
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1

        lower_bound = q1 - sensitivity * iqr
        upper_bound = q3 + sensitivity * iqr

        anomalies = []
        for _, row in df.iterrows():
            if row['value'] < lower_bound or row['value'] > upper_bound:
                anomalies.append(AnomalyPoint(
                    timestamp=row['timestamp'],
                    value=row['value'],
                    anomaly_score=abs(row['value'] - (q1 + q3) / 2) / iqr,
                    expected_range={
                        'lower': lower_bound,
                        'upper': upper_bound
                    }
                ))

        return anomalies

    @staticmethod
    def _zscore_anomaly_detection(df: pd.DataFrame, sensitivity: float) -> List[AnomalyPoint]:
        """Z-score 方法異常檢測"""
        values = df['value'].values
        z_scores = np.abs(stats.zscore(values))

        anomalies = []
        for i, (_, row) in enumerate(df.iterrows()):
            if z_scores[i] > sensitivity:
                anomalies.append(AnomalyPoint(
                    timestamp=row['timestamp'],
                    value=row['value'],
                    anomaly_score=z_scores[i],
                    expected_range={
                        'lower': np.mean(values) - sensitivity * np.std(values),
                        'upper': np.mean(values) + sensitivity * np.std(values)
                    }
                ))

        return anomalies
```

#### 任務 11.3：時間序列 API 端點
**預計時間：30 分鐘**

```python
# app/api/timeseries.py
from fastapi import APIRouter, HTTPException
from app.models.timeseries import (
    ForecastRequest, ForecastResult,
    AnomalyDetectionRequest, AnomalyDetectionResult
)
from app.services.timeseries import TimeSeriesService

router = APIRouter(prefix="/timeseries", tags=["時間序列分析"])

@router.post(
    "/forecast",
    response_model=ForecastResult,
    operation_id="forecast_timeseries",
    summary="時間序列預測",
    description="執行時間序列預測，支援線性趨勢、移動平均、指數平滑、ARIMA 模型"
)
async def forecast_timeseries(request: ForecastRequest) -> ForecastResult:
    """時間序列預測"""
    try:
        return TimeSeriesService.forecast_timeseries(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"時間序列預測時發生錯誤: {str(e)}"
        )

@router.post(
    "/anomaly-detection",
    response_model=AnomalyDetectionResult,
    operation_id="detect_timeseries_anomalies",
    summary="時間序列異常檢測",
    description="檢測時間序列中的異常點，支援統計方法、IQR、Z-score 檢測"
)
async def detect_anomalies(request: AnomalyDetectionRequest) -> AnomalyDetectionResult:
    """時間序列異常檢測"""
    try:
        return TimeSeriesService.detect_anomalies(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"異常檢測時發生錯誤: {str(e)}"
        )
```

### 第 12 天：快取系統與性能優化

#### 任務 12.1：Redis 快取層實作
**預計時間：2 小時**

```python
# app/services/cache.py
import json
import hashlib
from typing import Optional, Any, Dict
from redis.asyncio import Redis
import pickle
from datetime import timedelta

from app.core.settings import get_settings

settings = get_settings()

class CacheService:
    """Redis 快取服務"""

    def __init__(self):
        self.redis: Optional[Redis] = None

    async def connect(self):
        """連接 Redis"""
        self.redis = Redis.from_url(settings.redis_url, decode_responses=False)

    async def disconnect(self):
        """斷開 Redis 連接"""
        if self.redis:
            await self.redis.close()

    def _generate_key(self, prefix: str, data: Dict[str, Any]) -> str:
        """生成快取鍵值"""
        # 創建資料的哈希值
        data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
        data_hash = hashlib.md5(data_str.encode()).hexdigest()
        return f"{prefix}:{data_hash}"

    async def get_analysis_result(self, analysis_type: str, input_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """獲取快取的分析結果"""
        if not self.redis:
            return None

        key = self._generate_key(f"analysis:{analysis_type}", input_data)
        cached_data = await self.redis.get(key)

        if cached_data:
            return pickle.loads(cached_data)
        return None

    async def set_analysis_result(
        self,
        analysis_type: str,
        input_data: Dict[str, Any],
        result: Dict[str, Any],
        ttl_seconds: int = 3600  # 1小時過期
    ):
        """快取分析結果"""
        if not self.redis:
            return

        key = self._generate_key(f"analysis:{analysis_type}", input_data)
        serialized_result = pickle.dumps(result)
        await self.redis.setex(key, ttl_seconds, serialized_result)

    async def get_forecast_result(self, series_id: str, model_params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """獲取快取的預測結果"""
        if not self.redis:
            return None

        key = self._generate_key(f"forecast:{series_id}", model_params)
        cached_data = await self.redis.get(key)

        if cached_data:
            return pickle.loads(cached_data)
        return None

    async def set_forecast_result(
        self,
        series_id: str,
        model_params: Dict[str, Any],
        result: Dict[str, Any],
        ttl_seconds: int = 1800  # 30分鐘過期
    ):
        """快取預測結果"""
        if not self.redis:
            return

        key = self._generate_key(f"forecast:{series_id}", model_params)
        serialized_result = pickle.dumps(result)
        await self.redis.setex(key, ttl_seconds, serialized_result)

    async def invalidate_pattern(self, pattern: str):
        """清除符合模式的快取"""
        if not self.redis:
            return

        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)

    async def get_cache_stats(self) -> Dict[str, Any]:
        """獲取快取統計資訊"""
        if not self.redis:
            return {}

        info = await self.redis.info('memory')
        return {
            'used_memory': info.get('used_memory', 0),
            'used_memory_human': info.get('used_memory_human', '0B'),
            'total_keys': await self.redis.dbsize()
        }

# 全局快取實例
cache_service = CacheService()
```

#### 任務 12.2：快取裝飾器與整合
**預計時間：1.5 小時**

```python
# app/utils/cache_decorators.py
import asyncio
from functools import wraps
from typing import Callable, Any, Dict, Optional
import inspect

from app.services.cache import cache_service

def cache_analysis_result(
    analysis_type: str,
    ttl_seconds: int = 3600,
    cache_key_fields: Optional[list] = None
):
    """分析結果快取裝飾器"""

    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 提取快取鍵值所需的欄位
            if cache_key_fields:
                cache_data = {}
                # 從函數參數中提取指定欄位
                sig = inspect.signature(func)
                bound_args = sig.bind(*args, **kwargs)
                bound_args.apply_defaults()

                for field in cache_key_fields:
                    if hasattr(bound_args.arguments.get('request', {}), field):
                        cache_data[field] = getattr(bound_args.arguments['request'], field)
                    elif field in bound_args.arguments:
                        cache_data[field] = bound_args.arguments[field]
            else:
                # 使用所有參數作為快取鍵
                cache_data = dict(kwargs)
                if args:
                    cache_data['args'] = args

            # 嘗試從快取獲取結果
            cached_result = await cache_service.get_analysis_result(analysis_type, cache_data)
            if cached_result:
                return cached_result

            # 執行實際函數
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

            # 快取結果
            if hasattr(result, 'dict'):
                result_dict = result.dict()
            elif isinstance(result, dict):
                result_dict = result
            else:
                result_dict = result

            await cache_service.set_analysis_result(analysis_type, cache_data, result_dict, ttl_seconds)

            return result

        return wrapper
    return decorator

def cache_forecast_result(ttl_seconds: int = 1800):
    """預測結果快取裝飾器"""

    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 提取序列ID和模型參數
            request = kwargs.get('request') or (args[0] if args else None)
            if not request:
                return await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

            series_id = request.timeseries.series_id
            model_params = {
                'model_type': request.model_type,
                'forecast_periods': request.forecast_periods,
                'confidence_level': request.confidence_level
            }

            # 從快取獲取結果
            cached_result = await cache_service.get_forecast_result(series_id, model_params)
            if cached_result:
                return cached_result

            # 執行預測
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

            # 快取結果
            result_dict = result.dict() if hasattr(result, 'dict') else result
            await cache_service.set_forecast_result(series_id, model_params, result_dict, ttl_seconds)

            return result

        return wrapper
    return decorator
```

#### 任務 12.3：將快取整合到現有服務
**預計時間：30 分鐘**

```python
# 更新 app/services/statistics.py 添加快取支援
from app.utils.cache_decorators import cache_analysis_result

class StatisticsService:
    """統計分析服務 (含快取支援)"""

    @staticmethod
    @cache_analysis_result("descriptive_stats", ttl_seconds=3600, cache_key_fields=["data", "confidence_level"])
    def calculate_descriptive_statistics(
        data: List[Union[int, float]],
        confidence_level: float = 0.95
    ) -> DescriptiveStatistics:
        # 原有實作保持不變
        pass

    @staticmethod
    @cache_analysis_result("hypothesis_test", ttl_seconds=3600)
    def perform_hypothesis_test(request: HypothesisTestRequest) -> HypothesisTestResult:
        # 原有實作保持不變
        pass
```

```python
# 更新 app/services/timeseries.py 添加快取支援
from app.utils.cache_decorators import cache_forecast_result

class TimeSeriesService:
    """時間序列分析服務 (含快取支援)"""

    @staticmethod
    @cache_forecast_result(ttl_seconds=1800)
    def forecast_timeseries(request: ForecastRequest) -> ForecastResult:
        # 原有實作保持不變
        pass
```

### 第 13 天：機器學習模型管理基礎

#### 任務 13.1：模型存儲與版本管理
**預計時間：2.5 小時**

```python
# app/models/ml_models.py
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field
from enum import Enum
import uuid

class ModelType(str, Enum):
    """模型類型"""
    CLASSIFICATION = "classification"
    REGRESSION = "regression"
    CLUSTERING = "clustering"
    ANOMALY_DETECTION = "anomaly_detection"

class ModelStatus(str, Enum):
    """模型狀態"""
    TRAINING = "training"
    TRAINED = "trained"
    DEPLOYED = "deployed"
    DEPRECATED = "deprecated"
    FAILED = "failed"

class ModelMetadata(BaseModel):
    """模型元資料"""
    model_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="模型唯一識別碼")
    model_name: str = Field(..., description="模型名稱")
    model_type: ModelType = Field(..., description="模型類型")
    version: str = Field(..., description="模型版本")
    description: Optional[str] = Field(default=None, description="模型描述")

    # 訓練資訊
    training_data_hash: Optional[str] = Field(default=None, description="訓練資料哈希值")
    hyperparameters: Dict[str, Any] = Field(default={}, description="超參數")

    # 評估指標
    metrics: Dict[str, float] = Field(default={}, description="模型評估指標")

    # 狀態與時間戳
    status: ModelStatus = Field(default=ModelStatus.TRAINING, description="模型狀態")
    created_at: datetime = Field(default_factory=datetime.now, description="建立時間")
    updated_at: datetime = Field(default_factory=datetime.now, description="更新時間")

    # 儲存路徑
    model_path: Optional[str] = Field(default=None, description="模型檔案路徑")

    # 標籤
    tags: List[str] = Field(default=[], description="模型標籤")

class TrainingRequest(BaseModel):
    """模型訓練請求"""
    model_name: str = Field(..., description="模型名稱")
    model_type: ModelType = Field(..., description="模型類型")
    algorithm: str = Field(..., description="機器學習算法")

    # 訓練資料
    training_data: List[List[Union[int, float]]] = Field(..., description="訓練特徵資料")
    training_labels: List[Union[int, float, str]] = Field(..., description="訓練標籤")

    # 超參數
    hyperparameters: Dict[str, Any] = Field(default={}, description="超參數設定")

    # 驗證設定
    validation_split: float = Field(default=0.2, ge=0.1, le=0.5, description="驗證資料比例")
    random_state: int = Field(default=42, description="隨機種子")

    # 元資料
    description: Optional[str] = Field(default=None, description="訓練描述")
    tags: List[str] = Field(default=[], description="模型標籤")

class PredictionRequest(BaseModel):
    """預測請求"""
    model_id: str = Field(..., description="模型ID")
    input_data: List[List[Union[int, float]]] = Field(..., description="輸入特徵資料", min_items=1)
    return_probabilities: bool = Field(default=False, description="是否返回機率 (分類模型)")

class TrainingResult(BaseModel):
    """訓練結果"""
    model_id: str = Field(description="模型ID")
    model_metadata: ModelMetadata = Field(description="模型元資料")
    training_metrics: Dict[str, float] = Field(description="訓練指標")
    validation_metrics: Dict[str, float] = Field(description="驗證指標")
    training_time_seconds: float = Field(description="訓練時間(秒)")

class PredictionResult(BaseModel):
    """預測結果"""
    model_id: str = Field(description="使用的模型ID")
    predictions: List[Union[int, float, str]] = Field(description="預測結果")
    probabilities: Optional[List[List[float]]] = Field(default=None, description="預測機率 (如適用)")
    prediction_time_ms: float = Field(description="預測時間(毫秒)")
```

```python
# app/services/model_manager.py
import os
import json
import joblib
import hashlib
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC, SVR
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    mean_squared_error, mean_absolute_error, r2_score,
    silhouette_score, adjusted_rand_score
)

from app.models.ml_models import (
    ModelMetadata, TrainingRequest, PredictionRequest,
    TrainingResult, PredictionResult,
    ModelType, ModelStatus
)
from app.database.base import AsyncSessionLocal
from app.models.database import ModelTrainingLog

class ModelManager:
    """機器學習模型管理器"""

    def __init__(self, models_directory: str = "./models"):
        self.models_directory = models_directory
        self.loaded_models: Dict[str, Any] = {}  # 記憶體中的模型快取

        # 確保模型目錄存在
        os.makedirs(models_directory, exist_ok=True)

        # 支援的算法
        self.algorithms = {
            ModelType.CLASSIFICATION: {
                'random_forest': RandomForestClassifier,
                'logistic_regression': LogisticRegression,
                'svm': SVC
            },
            ModelType.REGRESSION: {
                'random_forest': RandomForestRegressor,
                'linear_regression': LinearRegression,
                'svr': SVR
            },
            ModelType.CLUSTERING: {
                'kmeans': KMeans
            }
        }

    def _calculate_data_hash(self, data: List[List[Union[int, float]]], labels: List[Union[int, float, str]]) -> str:
        """計算訓練資料的哈希值"""
        combined_data = str(data) + str(labels)
        return hashlib.md5(combined_data.encode()).hexdigest()

    def _get_model_path(self, model_id: str) -> str:
        """獲取模型檔案路徑"""
        return os.path.join(self.models_directory, f"{model_id}.joblib")

    def _get_metadata_path(self, model_id: str) -> str:
        """獲取元資料檔案路徑"""
        return os.path.join(self.models_directory, f"{model_id}_metadata.json")

    async def train_model(self, request: TrainingRequest) -> TrainingResult:
        """訓練模型"""
        start_time = time.time()

        # 準備資料
        X = np.array(request.training_data)
        y = np.array(request.training_labels)

        # 計算資料哈希
        data_hash = self._calculate_data_hash(request.training_data, request.training_labels)

        # 分割訓練和驗證資料
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=request.validation_split, random_state=request.random_state
        )

        # 選擇並初始化算法
        if request.model_type not in self.algorithms:
            raise ValueError(f"不支援的模型類型: {request.model_type}")

        if request.algorithm not in self.algorithms[request.model_type]:
            raise ValueError(f"模型類型 {request.model_type} 不支援算法 {request.algorithm}")

        algorithm_class = self.algorithms[request.model_type][request.algorithm]
        model = algorithm_class(**request.hyperparameters)

        # 訓練模型
        model.fit(X_train, y_train)

        # 計算訓練和驗證指標
        train_pred = model.predict(X_train)
        val_pred = model.predict(X_val)

        training_metrics = self._calculate_metrics(y_train, train_pred, request.model_type)
        validation_metrics = self._calculate_metrics(y_val, val_pred, request.model_type)

        # 建立模型元資料
        model_metadata = ModelMetadata(
            model_name=request.model_name,
            model_type=request.model_type,
            version="1.0.0",  # 簡化版本管理
            description=request.description,
            training_data_hash=data_hash,
            hyperparameters=request.hyperparameters,
            metrics=validation_metrics,
            status=ModelStatus.TRAINED,
            tags=request.tags
        )

        # 儲存模型
        model_path = self._get_model_path(model_metadata.model_id)
        joblib.dump(model, model_path)
        model_metadata.model_path = model_path

        # 儲存元資料
        metadata_path = self._get_metadata_path(model_metadata.model_id)
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(model_metadata.dict(), f, ensure_ascii=False, indent=2, default=str)

        # 載入到記憶體快取
        self.loaded_models[model_metadata.model_id] = model

        # 記錄到資料庫
        await self._log_training_to_database(model_metadata, training_metrics, validation_metrics)

        training_time = time.time() - start_time

        return TrainingResult(
            model_id=model_metadata.model_id,
            model_metadata=model_metadata,
            training_metrics=training_metrics,
            validation_metrics=validation_metrics,
            training_time_seconds=training_time
        )

    def _calculate_metrics(self, y_true, y_pred, model_type: ModelType) -> Dict[str, float]:
        """計算評估指標"""
        metrics = {}

        if model_type == ModelType.CLASSIFICATION:
            metrics['accuracy'] = accuracy_score(y_true, y_pred)
            metrics['precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)
            metrics['f1'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)

        elif model_type == ModelType.REGRESSION:
            metrics['mse'] = mean_squared_error(y_true, y_pred)
            metrics['mae'] = mean_absolute_error(y_true, y_pred)
            metrics['r2'] = r2_score(y_true, y_pred)
            metrics['rmse'] = np.sqrt(metrics['mse'])

        elif model_type == ModelType.CLUSTERING:
            # 聚類評估需要原始特徵資料，這裡簡化處理
            if len(np.unique(y_pred)) > 1:
                metrics['silhouette'] = silhouette_score(y_true, y_pred) if len(y_true.shape) > 1 else 0

        return metrics

    async def predict(self, request: PredictionRequest) -> PredictionResult:
        """執行預測"""
        start_time = time.time()

        # 載入模型
        model = await self._load_model(request.model_id)
        if model is None:
            raise ValueError(f"找不到模型: {request.model_id}")

        # 準備輸入資料
        X = np.array(request.input_data)

        # 執行預測
        predictions = model.predict(X)

        # 如果是分類模型且要求機率
        probabilities = None
        if request.return_probabilities and hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(X).tolist()

        prediction_time = (time.time() - start_time) * 1000  # 轉換為毫秒

        return PredictionResult(
            model_id=request.model_id,
            predictions=predictions.tolist(),
            probabilities=probabilities,
            prediction_time_ms=prediction_time
        )

    async def _load_model(self, model_id: str) -> Optional[Any]:
        """載入模型"""
        # 先檢查記憶體快取
        if model_id in self.loaded_models:
            return self.loaded_models[model_id]

        # 從檔案載入
        model_path = self._get_model_path(model_id)
        if not os.path.exists(model_path):
            return None

        try:
            model = joblib.load(model_path)
            self.loaded_models[model_id] = model
            return model
        except Exception:
            return None

    async def get_model_metadata(self, model_id: str) -> Optional[ModelMetadata]:
        """獲取模型元資料"""
        metadata_path = self._get_metadata_path(model_id)
        if not os.path.exists(metadata_path):
            return None

        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata_dict = json.load(f)
            return ModelMetadata(**metadata_dict)
        except Exception:
            return None

    async def list_models(self, model_type: Optional[ModelType] = None) -> List[ModelMetadata]:
        """列出所有模型"""
        models = []

        for filename in os.listdir(self.models_directory):
            if filename.endswith('_metadata.json'):
                model_id = filename.replace('_metadata.json', '')
                metadata = await self.get_model_metadata(model_id)
                if metadata and (model_type is None or metadata.model_type == model_type):
                    models.append(metadata)

        return sorted(models, key=lambda x: x.created_at, reverse=True)

    async def _log_training_to_database(
        self,
        metadata: ModelMetadata,
        training_metrics: Dict[str, float],
        validation_metrics: Dict[str, float]
    ):
        """記錄訓練到資料庫"""
        async with AsyncSessionLocal() as session:
            log_entry = ModelTrainingLog(
                model_name=metadata.model_name,
                model_version=metadata.version,
                training_data_hash=metadata.training_data_hash,
                hyperparameters=metadata.hyperparameters,
                metrics={
                    'training': training_metrics,
                    'validation': validation_metrics
                },
                model_path=metadata.model_path,
                status=metadata.status.value
            )
            session.add(log_entry)
            await session.commit()

# 全局模型管理器實例
model_manager = ModelManager()
```

#### 任務 13.2：機器學習 API 端點
**預計時間：1 小時**

```python
# app/api/machine_learning.py
from typing import List, Optional
from fastapi import APIRouter, HTTPException
from app.models.ml_models import (
    TrainingRequest, TrainingResult,
    PredictionRequest, PredictionResult,
    ModelMetadata, ModelType
)
from app.services.model_manager import model_manager

router = APIRouter(prefix="/ml", tags=["機器學習"])

@router.post(
    "/train",
    response_model=TrainingResult,
    operation_id="train_model",
    summary="訓練機器學習模型",
    description="訓練新的機器學習模型，支援分類、迴歸、聚類算法"
)
async def train_model(request: TrainingRequest) -> TrainingResult:
    """訓練機器學習模型"""
    try:
        return await model_manager.train_model(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"模型訓練時發生錯誤: {str(e)}"
        )

@router.post(
    "/predict",
    response_model=PredictionResult,
    operation_id="make_prediction",
    summary="執行模型預測",
    description="使用訓練好的模型進行預測"
)
async def make_prediction(request: PredictionRequest) -> PredictionResult:
    """執行模型預測"""
    try:
        return await model_manager.predict(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"預測時發生錯誤: {str(e)}"
        )

@router.get(
    "/models",
    response_model=List[ModelMetadata],
    operation_id="list_models",
    summary="列出所有模型",
    description="獲取所有已訓練模型的列表和元資料"
)
async def list_models(model_type: Optional[ModelType] = None) -> List[ModelMetadata]:
    """列出所有模型"""
    try:
        return await model_manager.list_models(model_type)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"列出模型時發生錯誤: {str(e)}"
        )

@router.get(
    "/models/{model_id}",
    response_model=ModelMetadata,
    operation_id="get_model_metadata",
    summary="獲取模型元資料",
    description="獲取指定模型的詳細元資料"
)
async def get_model_metadata(model_id: str) -> ModelMetadata:
    """獲取模型元資料"""
    try:
        metadata = await model_manager.get_model_metadata(model_id)
        if metadata is None:
            raise HTTPException(status_code=404, detail=f"找不到模型: {model_id}")
        return metadata
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"獲取模型元資料時發生錯誤: {str(e)}"
        )
```

### 第 14 天：整合測試與部署準備

#### 任務 14.1：更新主應用程式整合所有新功能
**預計時間：1.5 小時**

```python
# app/main.py - 完整版本
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.core.settings import get_settings
from app.database.base import engine, Base
from app.services.cache import cache_service
from app.middleware.mcp_monitoring import MCPMonitoringMiddleware

# 路由導入
from app.api.statistics import router as stats_router
from app.api.advanced_statistics import router as advanced_stats_router
from app.api.timeseries import router as timeseries_router
from app.api.machine_learning import router as ml_router

settings = get_settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """應用程式生命週期管理"""
    # 啟動時執行
    print("🚀 FastAPI MCP Server 啟動中...")

    # 建立資料庫表
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # 連接快取服務
    await cache_service.connect()

    print("✅ 所有服務已啟動")

    yield

    # 關閉時執行
    print("🛑 FastAPI MCP Server 關閉中...")
    await cache_service.disconnect()
    await engine.dispose()
    print("✅ 所有服務已關閉")

# 建立 FastAPI 應用程式
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="基於 Model Context Protocol 的統計分析與機器學習推論服務平台 - 第二週版本",
    debug=settings.debug,
    lifespan=lifespan
)

# 添加中介軟體
app.add_middleware(MCPMonitoringMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=settings.cors_allow_credentials,
    allow_methods=settings.cors_allow_methods,
    allow_headers=settings.cors_allow_headers,
)

# 註冊路由
app.include_router(stats_router)
app.include_router(advanced_stats_router)
app.include_router(timeseries_router)
app.include_router(ml_router)

# MCP 整合 (使用標準套件)
try:
    from fastapi_mcp import FastApiMCP

    mcp = FastApiMCP(
        app,
        name=settings.mcp_name,
        description=settings.mcp_description,
        include_tags=["統計分析", "進階統計分析", "時間序列分析", "機器學習"],
        describe_all_responses=True,
    )
    mcp.mount()
    print("✅ MCP 整合成功")

except ImportError:
    print("⚠️  fastapi-mcp 套件未安裝，跳過 MCP 整合")

# 基礎路由
@app.get("/", tags=["基礎"])
async def root():
    """根端點 - 服務狀態"""
    return {
        "service": settings.app_name,
        "version": settings.app_version,
        "status": "running",
        "features": [
            "descriptive_statistics",
            "hypothesis_testing",
            "correlation_analysis",
            "regression_analysis",
            "timeseries_forecasting",
            "anomaly_detection",
            "machine_learning",
            "model_management"
        ],
        "mcp_endpoint": "/mcp"
    }

@app.get("/health", tags=["基礎"])
async def health_check():
    """健康檢查端點"""
    cache_stats = await cache_service.get_cache_stats()
    return {
        "status": "healthy",
        "service": settings.app_name,
        "version": settings.app_version,
        "cache": cache_stats,
        "database": "connected",
        "timestamp": settings.app_version
    }

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.reload,
        log_level="debug" if settings.debug else "info",
    )
```

#### 任務 14.2：更新 Docker 配置
**預計時間：1 小時**

```yaml
# docker-compose.week2.yml
version: '3.8'

services:
  fastapi-mcp:
    build: .
    container_name: fastapi-mcp-server-week2
    ports:
      - "8000:8000"
    environment:
      - DEBUG=false
      - DATABASE_URL=postgresql+asyncpg://mcp_user:mcp_password@timescaledb:5432/fastapi_mcp
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - app_models:/code/models
      - app_logs:/code/logs
    depends_on:
      - timescaledb
      - redis
    restart: unless-stopped
    networks:
      - mcp_network

  timescaledb:
    image: timescale/timescaledb:2.14-pg16
    container_name: mcp-timescaledb
    restart: unless-stopped
    environment:
      - POSTGRES_DB=fastapi_mcp
      - POSTGRES_USER=mcp_user
      - POSTGRES_PASSWORD=mcp_password
    ports:
      - "5432:5432"
    volumes:
      - timescale_data:/var/lib/postgresql/data
      - ./scripts/init_timescale.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - mcp_network

  redis:
    image: redis:7-alpine
    container_name: mcp-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - mcp_network
    command: redis-server --appendonly yes

volumes:
  timescale_data:
  redis_data:
  app_models:
  app_logs:

networks:
  mcp_network:
    driver: bridge
```

#### 任務 14.3：第二週完整測試
**預計時間：30 分鐘**

```python
# tests/integration/test_week2_features.py
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

class TestWeek2Features:
    """第二週功能整合測試"""

    def test_advanced_statistics_correlation(self):
        """測試相關性分析"""
        payload = {
            "x_data": [1, 2, 3, 4, 5],
            "y_data": [2, 4, 6, 8, 10],
            "correlation_type": "pearson",
            "alpha": 0.05
        }

        response = client.post("/statistics/advanced/correlation", json=payload)
        assert response.status_code == 200

        data = response.json()
        assert "correlation_coefficient" in data
        assert "p_value" in data
        assert abs(data["correlation_coefficient"] - 1.0) < 0.01  # 完美正相關

    def test_regression_analysis(self):
        """測試迴歸分析"""
        payload = {
            "x_data": [1, 2, 3, 4, 5],
            "y_data": [2, 4, 6, 8, 10],
            "regression_type": "linear"
        }

        response = client.post("/statistics/advanced/regression", json=payload)
        assert response.status_code == 200

        data = response.json()
        assert "r_squared" in data
        assert "coefficients" in data
        assert data["r_squared"] > 0.9  # 高解釋力

    def test_timeseries_forecast(self):
        """測試時間序列預測"""
        payload = {
            "timeseries": {
                "series_id": "test_series",
                "data": [
                    {"timestamp": "2024-01-01T00:00:00", "value": 100},
                    {"timestamp": "2024-01-02T00:00:00", "value": 105},
                    {"timestamp": "2024-01-03T00:00:00", "value": 110},
                    {"timestamp": "2024-01-04T00:00:00", "value": 115},
                    {"timestamp": "2024-01-05T00:00:00", "value": 120},
                    {"timestamp": "2024-01-06T00:00:00", "value": 125},
                    {"timestamp": "2024-01-07T00:00:00", "value": 130},
                    {"timestamp": "2024-01-08T00:00:00", "value": 135},
                    {"timestamp": "2024-01-09T00:00:00", "value": 140},
                    {"timestamp": "2024-01-10T00:00:00", "value": 145}
                ]
            },
            "model_type": "linear_trend",
            "forecast_periods": 5
        }

        response = client.post("/timeseries/forecast", json=payload)
        assert response.status_code == 200

        data = response.json()
        assert "forecast_values" in data
        assert len(data["forecast_values"]) == 5

    def test_machine_learning_training(self):
        """測試機器學習模型訓練"""
        payload = {
            "model_name": "test_classifier",
            "model_type": "classification",
            "algorithm": "random_forest",
            "training_data": [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]],
            "training_labels": [0, 0, 1, 1, 1],
            "hyperparameters": {"n_estimators": 10, "random_state": 42}
        }

        response = client.post("/ml/train", json=payload)
        assert response.status_code == 200

        data = response.json()
        assert "model_id" in data
        assert "training_metrics" in data

        # 測試預測
        model_id = data["model_id"]
        prediction_payload = {
            "model_id": model_id,
            "input_data": [[1, 2], [5, 6]]
        }

        pred_response = client.post("/ml/predict", json=prediction_payload)
        assert pred_response.status_code == 200

        pred_data = pred_response.json()
        assert "predictions" in pred_data
        assert len(pred_data["predictions"]) == 2

    def test_health_check_enhanced(self):
        """測試增強的健康檢查"""
        response = client.get("/health")
        assert response.status_code == 200

        data = response.json()
        assert data["status"] == "healthy"
        assert "cache" in data
        assert "database" in data
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u5206\u6790\u7b2c\u4e00\u9031\u4efb\u52d9\u5b8c\u6210\u60c5\u6cc1\u8207\u4f01\u756b\u66f8\u5167\u5bb9\u5c0d\u6bd4", "status": "completed", "activeForm": "\u5206\u6790\u7b2c\u4e00\u9031\u4efb\u52d9\u5b8c\u6210\u60c5\u6cc1\u8207\u4f01\u756b\u66f8\u5167\u5bb9\u5c0d\u6bd4"}, {"content": "\u6aa2\u67e5\u7b2c\u4e00\u9031\u5be6\u4f5c\u8207\u4f01\u756b\u66f8\u898f\u683c\u7684\u4e00\u81f4\u6027", "status": "completed", "activeForm": "\u6aa2\u67e5\u7b2c\u4e00\u9031\u5be6\u4f5c\u8207\u4f01\u756b\u66f8\u898f\u683c\u7684\u4e00\u81f4\u6027"}, {"content": "\u6839\u64da\u4f01\u756b\u66f8\u5236\u5b9a\u7b2c\u4e8c\u9031\u4efb\u52d9\u7d30\u5206\u8a08\u756b", "status": "completed", "activeForm": "\u6839\u64da\u4f01\u756b\u66f8\u5236\u5b9a\u7b2c\u4e8c\u9031\u4efb\u52d9\u7d30\u5206\u8a08\u756b"}, {"content": "\u9a57\u8b49 fastapi-mcp \u6574\u5408\u65b9\u5f0f\u7684\u6b63\u78ba\u6027", "status": "completed", "activeForm": "\u9a57\u8b49 fastapi-mcp \u6574\u5408\u65b9\u5f0f\u7684\u6b63\u78ba\u6027"}]
